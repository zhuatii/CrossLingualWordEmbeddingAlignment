{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "d3fed8cf-1447-49f2-8c5d-05facb20f5c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipreqs  --force ./\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Not scanning for jupyter notebooks.\n",
      "WARNING: Import named \"numpy\" not found locally. Trying to resolve it at the PyPI server.\n",
      "WARNING: Import named \"numpy\" was resolved to \"numpy:2.1.1\" package (https://pypi.org/project/numpy/).\n",
      "Please, verify manually the final list of requirements.txt to avoid possible dependency confusions.\n",
      "WARNING: Import named \"pandas\" not found locally. Trying to resolve it at the PyPI server.\n",
      "WARNING: Import named \"pandas\" was resolved to \"pandas:2.2.3\" package (https://pypi.org/project/pandas/).\n",
      "Please, verify manually the final list of requirements.txt to avoid possible dependency confusions.\n",
      "WARNING: Import named \"scipy\" not found locally. Trying to resolve it at the PyPI server.\n",
      "WARNING: Import named \"scipy\" was resolved to \"scipy:1.14.1\" package (https://pypi.org/project/scipy/).\n",
      "Please, verify manually the final list of requirements.txt to avoid possible dependency confusions.\n",
      "INFO: Successfully saved requirements file in ./requirements.txt\n"
     ]
    }
   ],
   "source": [
    "### To maintain the packages and the the used versions\n",
    "# !pip install pipreqsnb\n",
    "!pipreqsnb --force ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ff4e0fe-b6ba-4792-b09a-8a54a388d224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e90aa0-cbc5-4907-94a9-d4575ba31808",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "## Loading data : English to Hindi translation pairs from the MUSE dataset by Meta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226b99af-a11c-4100-bd12-a8b2041968c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e5d270e-ce6f-44eb-a3f9-dfa45c8bdeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"data/train.txt\"\n",
    "test_path = \"data/test.txt\"\n",
    "data_path = \"data/en-hi.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50bee5-c3af-4e59-84c4-ef1d69182dae",
   "metadata": {},
   "source": [
    "We are loading the text pairs into a pandas dataframe for easy access. Reading with Pandas was chosen over directly loading from the file as the system could not recognise the Devanagri script if loaded directly and was throwing errors. \n",
    "\n",
    "<!-- Separate train and test files were downloaded from the github repository and loaded accordingly. The train dataset contains 8704 text pairs and the test set has 2032 pairs. Sample of the dataset is also shown. Each entity of the dataframe is a string. The dataframes are then converted to dictionaries for ease of access. -->\n",
    "\n",
    "The entire dataset contains 38216 pairs of english-hindi translations. 80% of the data is to be used for training and the rest 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b1dee2-7221-4680-b90e-10f4cb1e8415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 38216 entries, 0 to 38220\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       38216 non-null  object\n",
      " 1   1       38216 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 895.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(data_path, sep = '\\t', header = None)\n",
    "data_df = data_df.dropna()\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee8231df-cac8-42e9-809d-66583faf7881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 30568 entries, 0 to 30572\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       30568 non-null  object\n",
      " 1   1       30568 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 716.4+ KB\n",
      "Train Data Info None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7648 entries, 30573 to 38220\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       7648 non-null   object\n",
      " 1   1       7648 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 179.2+ KB\n",
      "Test Data Info None\n"
     ]
    }
   ],
   "source": [
    "train_df = data_df.loc[:len(data_df)*0.8]\n",
    "test_df = data_df.loc[len(data_df)*0.8:]\n",
    "\n",
    "print(\"Train Data Info\", train_df.info())\n",
    "print(\"Test Data Info\", test_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "8db561b2-03c1-470b-a62c-b2bb87a0ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(train_path, sep = '\\t', header = None)\n",
    "# print(train_df.info())\n",
    "\n",
    "# test_df = pd.read_csv(test_path, sep = '\\t', header = None)\n",
    "# print(test_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f3ef739-26c1-465e-85a7-3404565d6bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>और</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was</td>\n",
       "      <td>था</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>was</td>\n",
       "      <td>थी</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>for</td>\n",
       "      <td>लिये</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>that</td>\n",
       "      <td>उस</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0   and    और\n",
       "1   was    था\n",
       "2   was    थी\n",
       "3   for  लिये\n",
       "4  that    उस"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dfb29c-10cf-44ff-aacf-e7beb0074c8a",
   "metadata": {},
   "source": [
    "## Loading pre-trained Fasttext monolingual embeddings\n",
    "\n",
    "To get the word vectors for different words in both Hindi and English, the embedding models are downloaded from [here](https://fasttext.cc/docs/en/pretrained-vectors.html). The word vectors for all the words from the dataset, both the train and test sets, are generated and stored for future references in two separate lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cb23ad0-c069-4101-aa9b-b30790558585",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install fasttext-wheel\n",
    "import fasttext\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38d4c536-3914-4f17-bf22-441eb9e22094",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_embed = fasttext.load_model('Embeddings/wiki.hi.bin')     ### Embedding model for Hindi words\n",
    "english_embed = fasttext.load_model('Embeddings/wiki.en.bin')   ### Embedding model for English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32475b4a-d375-4450-8e5c-b332cde72bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = list(train_df[0]) + list(test_df[0])            ### All the English words in corpus\n",
    "hindi_words = list(train_df[1]) + list(test_df[1])              ### All the Hindi words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a7eedbe-b808-4ea6-a479-443db209aa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38216 38216\n"
     ]
    }
   ],
   "source": [
    "print(len(english_words), len(hindi_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe958ea7-849e-493a-897a-dd4245cedb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use of dictionaries is discarded to preserve the duplicate entries\n",
    "# english_vectors = {word : english_embed.get_word_vector(word) for word in english_words}\n",
    "# hindi_vectors = {word: hindi_embed.get_word_vector(word) for word in hindi_words}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "edff711f-51d2-425f-9df4-b6592db5c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To load the word vectors. To create the embedding word vectors from scratch, run the commented out lines in this cell\n",
    "\n",
    "english_vectors = joblib.load(\"vars/english_vectors\")\n",
    "hindi_vectors = joblib.load(\"vars/hindi_vectors\")\n",
    "\n",
    "# english_vectors = [english_embed.get_word_vector(word) for word in english_words]\n",
    "# hindi_vectors = [hindi_embed.get_word_vector(word) for word in hindi_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e9a119b-e315-4579-a81b-03e5213fe96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To store the list of word vectors\n",
    "\n",
    "# joblib.dump(english_vectors, \"vars/english_vectors\")\n",
    "# joblib.dump(hindi_vectors, \"vars/hindi_vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "21755158-3970-4fc4-b46f-2a2706cd6a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Dimension of embeddings\n",
    "\n",
    "dim = len(list(english_vectors)[0])\n",
    "dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a9d07-1d4a-44d3-a204-2472b1e22cb5",
   "metadata": {},
   "source": [
    "# Embedding Alignment: Applying orthogonal Procrustes method\n",
    "\n",
    "For the calculation of the solution of the Procrustes alignment, the train set containing embedding word vectors is constructed into matrices of dimensions (*numSamples , dimension of embedding*), i.e. (8704, 300) for our case here.\n",
    "\n",
    "The Procrustes method utilises Singular Value Decomposition to compute the best transformation to align the source matrix to the target matrix. Once the solution **W** is computed, the transformation is applied to the embedding of an English word ($s_e$) to get a resulting transformed vector ($W \\cdot s_e$). According to [Conneau et al.](https://arxiv.org/pdf/1710.04087), the corresponding word from the resulting embedding is given by the Hindi word ($h$), whose embedding has the highest cosine similarity with the transformed source vector embedding.\n",
    "\n",
    "$$\r\n",
    "t = \\arg\\max_t \\cos(W \\cdot x_s, y_t)\r\n",
    "$$\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "010f9178-b1e4-43bd-bd8c-691f277b821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import orthogonal_procrustes\n",
    "from scipy.linalg import svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0108a9ef-c34a-462b-8602-6fb52acf6ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMatrices(n, english_vectors, hindi_vectors):\n",
    "    english_matrix = np.empty((0, dim))\n",
    "    hindi_matrix = np.empty((0, dim))\n",
    "    \n",
    "    for i in range(n):\n",
    "        english_matrix = np.vstack([english_matrix, english_vectors[i].reshape((1, 300))])\n",
    "        hindi_matrix = np.vstack([hindi_matrix, hindi_vectors[i].reshape((1, 300))])\n",
    "\n",
    "    return english_matrix, hindi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7359467-d48c-418b-bc36-29acb63102d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To save computation time, precomputed matrices are stored for ease of execution. To recompute the matrices, line commented below\n",
    "\n",
    "# english_matrix, hindi_matrix = createMatrices(len(train_data), english_vectors, hindi_vectors)\n",
    "\n",
    "english_matrix = joblib.load(\"vars/english_matrix\")\n",
    "hindi_matrix = joblib.load(\"vars/hindi_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc36f376-b545-4cdc-9d4f-1675284596a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code to store the variables\n",
    "\n",
    "# joblib.dump(english_matrix, \"vars/english_matrix\")\n",
    "# joblib.dump(hindi_matrix, \"vars/hindi_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02b887cb-1607-4c33-8f7e-de5e8f692a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30568, 300), (30568, 300))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_matrix.shape, hindi_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24575e1a-baf3-469b-9701-670b53cbc26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = hindi_matrix @ (english_matrix.T)\n",
    "# U, s, V = svd(A, full_matrices=0)\n",
    "# R = U @ V.T\n",
    "# W = np.multiply((1 + 0.01), R) - np.multiply(0.01, R @ R.T) @ R\n",
    "\n",
    "#### The above is the exact implementation of the code from the paper. However due to the large sizes of the English and Hindi matrices, the SVD \n",
    "#### was not being performed\n",
    "\n",
    "W, A = orthogonal_procrustes(english_matrix, hindi_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3adb4d54-a733-42da-999f-c942abb53119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09325ca3-bdb2-49d4-bb8a-c483314295fc",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73da382-869e-411d-8fca-48e41140c9bf",
   "metadata": {},
   "source": [
    "## Function for translating English words to Hindi\n",
    "\n",
    "The function takes in the English word to be translated, the transformation matrix from the Procrustes problem with orthogonality, the model which converts English words to their corresponding word vector and all the Hindi word vectors, to return the Hindi translation of the English word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9832dc66-14cf-4af0-9200-562b900fc6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateWord(english_word, R, english_model, hindi_vectors):        \n",
    "\n",
    "    english_vector = english_embed.get_word_vector(english_word)\n",
    "    translation_vector = np.dot(english_vector, R)\n",
    "    i = max(np.arange(0, len(hindi_vectors)-1), key=lambda i: np.dot(hindi_vectors[i], translation_vector))\n",
    "    \n",
    "    return hindi_words[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "721ae753-fbf5-40d0-87e9-cc82b8d25d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'फ़रवरी'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translateWord('november', W, english_embed, hindi_vectors) ### The translation is not very accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "68c14f9c-56d0-4413-8b8d-9e87f12301fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calAccuracy(df):\n",
    "    \n",
    "    hindi_translated = []\n",
    "    \n",
    "    for word in df[0]:\n",
    "        t = translateWord(word, W, english_embed, hindi_vectors)\n",
    "        hindi_translated.append(t)\n",
    "        \n",
    "    trueHindi = list(df[1])\n",
    "    preds = []\n",
    "    \n",
    "    for i in range(len(trueHindi)):\n",
    "        if trueHindi[i] == hindi_translated[i]:\n",
    "            preds.append(1)\n",
    "        else:\n",
    "            preds.append(0)\n",
    "            \n",
    "    return (sum(preds)/len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5688ecfe-73ca-481f-864a-ee1ca6cdbc51",
   "metadata": {},
   "source": [
    "## Accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "8ffbc7b7-87a1-441b-8e49-86c05046ca60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of translation by the model on the training set is 0.1273\n",
      "The accuracy of translation by the model on the test set is 0.0088\n"
     ]
    }
   ],
   "source": [
    "trainAcc = calAccuracy(train_df)\n",
    "testAcc = calAccuracy(test_df)\n",
    "\n",
    "print(\"The accuracy of translation by the model on the training set is {:.4f}\".format(trainAcc))\n",
    "print(\"The accuracy of translation by the model on the test set is {:.4f}\".format(testAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c319a1-5696-4710-9826-59dc978d47d2",
   "metadata": {},
   "source": [
    "## Precision@k\n",
    "\n",
    "Precsion@k metric counts the number of true predictions in the top 'k' predicted words as translations for the given word. We are interested in Precision@1 and Precision@5 metrics. Precision@1 could also be considered as accuracy as we are considering the top 1 prediction as the translation. The computation of Precision@5 is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc895fd0-1daa-453a-9700-54f14629ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topTranslateWords(english_word, R , english_model, hindi_vectors, k):        \n",
    "\n",
    "    english_vector = english_embed.get_word_vector(english_word)\n",
    "    translation_vector = np.dot(english_vector, R)\n",
    "    indices = sorted(np.arange(0, len(hindi_vectors)-1), key=lambda i: np.dot(hindi_vectors[i], translation_vector), reverse = True)[:k]\n",
    "    \n",
    "    return [hindi_words[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f9c6c852-ad4d-4478-aa02-4891ed19c479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['फ़रवरी', 'फ़रवरी', 'नवम्बर', 'नवम्बर', 'दिसम्बर']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topTranslateWords('november', W, english_embed, hindi_vectors, 5)     #### the dataset is not perfect and has english words as hindi translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e6844271-ff25-490e-a7ba-f79fd14c1dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calPrecision(df, k):\n",
    "    topAns = []\n",
    "    \n",
    "    for word in df[0]:\n",
    "        t = topTranslateWords(word, W, english_embed, hindi_vectors, k)\n",
    "        topAns.append(t)\n",
    "        \n",
    "    trueHindi = list(df[1])\n",
    "    preds = []\n",
    "    \n",
    "    for i in range(len(trueHindi)):\n",
    "        if trueHindi[i] in topAns[i]:\n",
    "            preds.append(1)\n",
    "        else:\n",
    "            preds.append(0)\n",
    "            \n",
    "    return (sum(preds)/len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "226fcb56-f572-4b3a-a0aa-054acb0a3036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of translation by the model on the training set is 0.2575\n",
      "The accuracy of translation by the model on the test set is 0.0395\n"
     ]
    }
   ],
   "source": [
    "trainAcc = calPrecision(train_df, 5)\n",
    "testAcc = calPrecision(test_df, 5)\n",
    "\n",
    "print(\"The accuracy of translation by the model on the training set is {:.4f}\".format(trainAcc))\n",
    "print(\"The accuracy of translation by the model on the test set is {:.4f}\".format(testAcc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
